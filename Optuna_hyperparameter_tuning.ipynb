{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyN5AngSo8q3IUO5EsOYINVn",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/harishjungshahi/Deep-Learning/blob/main/Optuna_hyperparameter_tuning.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Optuna HyperParameter Tuning"
      ],
      "metadata": {
        "id": "62Zr9uQe8zUt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install optuna"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mSApyU1I9w-W",
        "outputId": "3f44eb40-4632-44af-eca7-8bb329ae6d31"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting optuna\n",
            "  Downloading optuna-4.4.0-py3-none-any.whl.metadata (17 kB)\n",
            "Collecting alembic>=1.5.0 (from optuna)\n",
            "  Downloading alembic-1.16.2-py3-none-any.whl.metadata (7.3 kB)\n",
            "Collecting colorlog (from optuna)\n",
            "  Downloading colorlog-6.9.0-py3-none-any.whl.metadata (10 kB)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (from optuna) (2.0.2)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from optuna) (24.2)\n",
            "Requirement already satisfied: sqlalchemy>=1.4.2 in /usr/local/lib/python3.11/dist-packages (from optuna) (2.0.41)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (from optuna) (4.67.1)\n",
            "Requirement already satisfied: PyYAML in /usr/local/lib/python3.11/dist-packages (from optuna) (6.0.2)\n",
            "Requirement already satisfied: Mako in /usr/lib/python3/dist-packages (from alembic>=1.5.0->optuna) (1.1.3)\n",
            "Requirement already satisfied: typing-extensions>=4.12 in /usr/local/lib/python3.11/dist-packages (from alembic>=1.5.0->optuna) (4.14.0)\n",
            "Requirement already satisfied: greenlet>=1 in /usr/local/lib/python3.11/dist-packages (from sqlalchemy>=1.4.2->optuna) (3.2.3)\n",
            "Downloading optuna-4.4.0-py3-none-any.whl (395 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m395.9/395.9 kB\u001b[0m \u001b[31m21.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading alembic-1.16.2-py3-none-any.whl (242 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m242.7/242.7 kB\u001b[0m \u001b[31m16.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading colorlog-6.9.0-py3-none-any.whl (11 kB)\n",
            "Installing collected packages: colorlog, alembic, optuna\n",
            "Successfully installed alembic-1.16.2 colorlog-6.9.0 optuna-4.4.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "T0pieLF98tIk"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torch.nn.functional as F\n",
        "from torch.utils.data import DataLoader, TensorDataset\n",
        "import optuna\n",
        "from sklearn.datasets import make_classification\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Load Syntetically Generated Dataset"
      ],
      "metadata": {
        "id": "PG6v4G5x96K4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Generate Synthetic Datasets\n",
        "X, y = make_classification(n_samples=1000, n_features=20, n_classes=2, random_state=42)\n",
        "X_train, X_val, y_train, y_val = train_test_split(X,y,test_size=0.2, random_state=42)"
      ],
      "metadata": {
        "id": "o84DScSA-UDv"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X_train.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BFePBs80-o-F",
        "outputId": "1a6ed21d-7da4-4f64-9056-a0967402eea4"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(800, 20)"
            ]
          },
          "metadata": {},
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "X_train[0]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GfG-RE5U-s1U",
        "outputId": "fa11abbe-7071-4de1-9b9c-c77a68f9c5d2"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([ 0.50363664, -1.51368248, -0.46907062,  1.90176571, -0.87064279,\n",
              "        1.82004715,  1.66291365,  1.29105223, -0.16713608, -1.04718436,\n",
              "        1.43003039,  0.20104766,  1.27577182, -1.13260729,  1.75008532,\n",
              "       -1.4089039 ,  0.03301588, -0.80340946, -1.31410638,  1.41209637])"
            ]
          },
          "metadata": {},
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "y_train[0]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "c50OhiGv-3Gb",
        "outputId": "add8b46c-f5e6-4200-a275-60d5b67fc774"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "np.int64(1)"
            ]
          },
          "metadata": {},
          "execution_count": 18
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Convert to pytorch tensors"
      ],
      "metadata": {
        "id": "Rj0CpwZL_Bor"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "X_train, y_train = torch.tensor(X_train, dtype=torch.float32), torch.tensor(y_train, dtype=torch.long)\n",
        "X_val, y_val = torch.tensor(X_val, dtype=torch.float32), torch.tensor(y_val, dtype=torch.long)"
      ],
      "metadata": {
        "id": "HICJkL2e-7yb"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Define a Neural Network"
      ],
      "metadata": {
        "id": "0xbvzQzoAbzG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class SimpleNN(nn.Module):\n",
        "  def __init__(self, input_dim, hidden_dim):\n",
        "    super().__init__()\n",
        "    self.network = nn.Sequential(\n",
        "        nn.Linear(input_dim, hidden_dim),\n",
        "        nn.ReLU(),\n",
        "        nn.Linear(hidden_dim, 2) #Output Layer for Binary Classification\n",
        "    )\n",
        "\n",
        "  def forward(self,x):\n",
        "    return self.network(x)"
      ],
      "metadata": {
        "id": "RQ41dN5fAWkh"
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Perform HyperParameter Tuning"
      ],
      "metadata": {
        "id": "qdKuAlLaBbJo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 3. Define the objective function for Optuna\n",
        "def objective(trial):\n",
        "    # Suggest values for the hyperparameters\n",
        "    learning_rate = trial.suggest_loguniform('learning_rate', 1e-4, 1e-1)\n",
        "    hidden_dim = trial.suggest_int('hidden_dim', 16, 128)\n",
        "\n",
        "    # Model, loss, optimizer\n",
        "    model = SimpleNN(input_dim=20, hidden_dim=hidden_dim)\n",
        "    criterion = nn.CrossEntropyLoss()\n",
        "    optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
        "\n",
        "    # Training loop\n",
        "    epochs = 20\n",
        "    batch_size = 32\n",
        "    train_loader = DataLoader(TensorDataset(X_train, y_train), batch_size=batch_size, shuffle=True)\n",
        "    val_loader = DataLoader(TensorDataset(X_val, y_val), batch_size=batch_size)\n",
        "\n",
        "    for epoch in range(epochs):\n",
        "        model.train()\n",
        "        for batch_X, batch_y in train_loader:\n",
        "            optimizer.zero_grad()\n",
        "            outputs = model(batch_X)\n",
        "            loss = criterion(outputs, batch_y)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "    # Validation accuracy\n",
        "    model.eval()\n",
        "    correct = 0\n",
        "    total = 0\n",
        "    with torch.no_grad():\n",
        "        for batch_X, batch_y in val_loader:\n",
        "            outputs = model(batch_X)\n",
        "            _, predicted = torch.max(outputs, 1)\n",
        "            total += batch_y.size(0)\n",
        "            correct += (predicted == batch_y).sum().item()\n",
        "\n",
        "    accuracy = correct / total\n",
        "    return accuracy\n",
        "\n",
        "# 4. Run the Optuna optimization\n",
        "study = optuna.create_study(direction=\"maximize\")\n",
        "study.optimize(objective, n_trials=20)\n",
        "\n",
        "# 5. Print the best hyperparameters\n",
        "print(\"Best hyperparameters found:\")\n",
        "print(study.best_params)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "A7EJgve1DG57",
        "outputId": "945417dc-72b4-4e0f-c815-b67d9efafcc9"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2025-06-19 11:57:59,874] A new study created in memory with name: no-name-6c00f5e7-ac20-4011-af8c-bb5350e89105\n",
            "[I 2025-06-19 11:58:07,005] Trial 0 finished with value: 0.77 and parameters: {'learning_rate': 0.04814286194067602, 'hidden_dim': 65}. Best is trial 0 with value: 0.77.\n",
            "/tmp/ipython-input-23-364525301.py:4: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
            "  learning_rate = trial.suggest_loguniform('learning_rate', 1e-4, 1e-1)\n",
            "[I 2025-06-19 11:58:07,766] Trial 1 finished with value: 0.87 and parameters: {'learning_rate': 0.00023683515062755778, 'hidden_dim': 91}. Best is trial 1 with value: 0.87.\n",
            "/tmp/ipython-input-23-364525301.py:4: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
            "  learning_rate = trial.suggest_loguniform('learning_rate', 1e-4, 1e-1)\n",
            "[I 2025-06-19 11:58:08,547] Trial 2 finished with value: 0.825 and parameters: {'learning_rate': 0.00018767049651907206, 'hidden_dim': 22}. Best is trial 1 with value: 0.87.\n",
            "/tmp/ipython-input-23-364525301.py:4: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
            "  learning_rate = trial.suggest_loguniform('learning_rate', 1e-4, 1e-1)\n",
            "[I 2025-06-19 11:58:09,307] Trial 3 finished with value: 0.845 and parameters: {'learning_rate': 0.021263563461411894, 'hidden_dim': 105}. Best is trial 1 with value: 0.87.\n",
            "/tmp/ipython-input-23-364525301.py:4: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
            "  learning_rate = trial.suggest_loguniform('learning_rate', 1e-4, 1e-1)\n",
            "[I 2025-06-19 11:58:10,005] Trial 4 finished with value: 0.83 and parameters: {'learning_rate': 0.0248095122701534, 'hidden_dim': 23}. Best is trial 1 with value: 0.87.\n",
            "[I 2025-06-19 11:58:11,017] Trial 5 finished with value: 0.815 and parameters: {'learning_rate': 0.01554890295137773, 'hidden_dim': 125}. Best is trial 1 with value: 0.87.\n",
            "/tmp/ipython-input-23-364525301.py:4: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
            "  learning_rate = trial.suggest_loguniform('learning_rate', 1e-4, 1e-1)\n",
            "[I 2025-06-19 11:58:12,063] Trial 6 finished with value: 0.845 and parameters: {'learning_rate': 0.0021994461822763497, 'hidden_dim': 94}. Best is trial 1 with value: 0.87.\n",
            "/tmp/ipython-input-23-364525301.py:4: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
            "  learning_rate = trial.suggest_loguniform('learning_rate', 1e-4, 1e-1)\n",
            "[I 2025-06-19 11:58:13,040] Trial 7 finished with value: 0.82 and parameters: {'learning_rate': 0.0004091845227195921, 'hidden_dim': 20}. Best is trial 1 with value: 0.87.\n",
            "/tmp/ipython-input-23-364525301.py:4: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
            "  learning_rate = trial.suggest_loguniform('learning_rate', 1e-4, 1e-1)\n",
            "[I 2025-06-19 11:58:13,775] Trial 8 finished with value: 0.82 and parameters: {'learning_rate': 0.008026366478741026, 'hidden_dim': 96}. Best is trial 1 with value: 0.87.\n",
            "/tmp/ipython-input-23-364525301.py:4: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
            "  learning_rate = trial.suggest_loguniform('learning_rate', 1e-4, 1e-1)\n",
            "[I 2025-06-19 11:58:14,521] Trial 9 finished with value: 0.785 and parameters: {'learning_rate': 0.03970270017136766, 'hidden_dim': 78}. Best is trial 1 with value: 0.87.\n",
            "[I 2025-06-19 11:58:15,334] Trial 10 finished with value: 0.88 and parameters: {'learning_rate': 0.001182946789066223, 'hidden_dim': 53}. Best is trial 10 with value: 0.88.\n",
            "/tmp/ipython-input-23-364525301.py:4: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
            "  learning_rate = trial.suggest_loguniform('learning_rate', 1e-4, 1e-1)\n",
            "[I 2025-06-19 11:58:16,063] Trial 11 finished with value: 0.88 and parameters: {'learning_rate': 0.0010331349767484777, 'hidden_dim': 54}. Best is trial 10 with value: 0.88.\n",
            "[I 2025-06-19 11:58:16,848] Trial 12 finished with value: 0.86 and parameters: {'learning_rate': 0.001195803024331272, 'hidden_dim': 47}. Best is trial 10 with value: 0.88.\n",
            "/tmp/ipython-input-23-364525301.py:4: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
            "  learning_rate = trial.suggest_loguniform('learning_rate', 1e-4, 1e-1)\n",
            "[I 2025-06-19 11:58:17,596] Trial 13 finished with value: 0.87 and parameters: {'learning_rate': 0.0007636170946934223, 'hidden_dim': 48}. Best is trial 10 with value: 0.88.\n",
            "/tmp/ipython-input-23-364525301.py:4: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
            "  learning_rate = trial.suggest_loguniform('learning_rate', 1e-4, 1e-1)\n",
            "[I 2025-06-19 11:58:18,572] Trial 14 finished with value: 0.825 and parameters: {'learning_rate': 0.004432481755658685, 'hidden_dim': 47}. Best is trial 10 with value: 0.88.\n",
            "/tmp/ipython-input-23-364525301.py:4: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
            "  learning_rate = trial.suggest_loguniform('learning_rate', 1e-4, 1e-1)\n",
            "[I 2025-06-19 11:58:19,433] Trial 15 finished with value: 0.87 and parameters: {'learning_rate': 0.0015223573164420583, 'hidden_dim': 63}. Best is trial 10 with value: 0.88.\n",
            "/tmp/ipython-input-23-364525301.py:4: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
            "  learning_rate = trial.suggest_loguniform('learning_rate', 1e-4, 1e-1)\n",
            "[I 2025-06-19 11:58:20,143] Trial 16 finished with value: 0.855 and parameters: {'learning_rate': 0.0005491340859851806, 'hidden_dim': 37}. Best is trial 10 with value: 0.88.\n",
            "[I 2025-06-19 11:58:20,931] Trial 17 finished with value: 0.82 and parameters: {'learning_rate': 0.00423929600311933, 'hidden_dim': 75}. Best is trial 10 with value: 0.88.\n",
            "/tmp/ipython-input-23-364525301.py:4: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
            "  learning_rate = trial.suggest_loguniform('learning_rate', 1e-4, 1e-1)\n",
            "[I 2025-06-19 11:58:21,719] Trial 18 finished with value: 0.805 and parameters: {'learning_rate': 0.00012430629976894843, 'hidden_dim': 58}. Best is trial 10 with value: 0.88.\n",
            "/tmp/ipython-input-23-364525301.py:4: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
            "  learning_rate = trial.suggest_loguniform('learning_rate', 1e-4, 1e-1)\n",
            "[I 2025-06-19 11:58:22,504] Trial 19 finished with value: 0.875 and parameters: {'learning_rate': 0.0010090358696001234, 'hidden_dim': 37}. Best is trial 10 with value: 0.88.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Best hyperparameters found:\n",
            "{'learning_rate': 0.001182946789066223, 'hidden_dim': 53}\n"
          ]
        }
      ]
    }
  ]
}